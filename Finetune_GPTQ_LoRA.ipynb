{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The aim of this notebook is to demonstrate how to use Hugging Face's TRL (Transformers Reinforcement Learning) library to finetune a 4bit GPTQ quantized model. More details on how to quantize a model using GPTQ can be found in my medium article [here](https://medium.com/@arunsreekuttan1996/quantizing-qwen2-vl-models-with-gptqmodel-a-complete-guide-for-multi-modal-model-compression-and-f329ea18a17b)\n",
    "\n",
    "If you are okay with using a 4bit nf4 (bitsandbytes) quantized model, you can use either Unsloth's fine-tuning notebooks or this notebook with changing the quantization configurations, which will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "We'll be using the `unsloth/LaTeX_OCR` dataset from Hugging Face. The goal is to convert these images into a computer readable form - ie in LaTeX form, so we can render it or to use in LaTeX documents. 80% of the dataset is used for training and 20% for testing.\n",
    "\n",
    "You can access the dataset [here](https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Funsloth%2FLaTeX_OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset,test_dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=['train[:80%]', 'test[:20%]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s take a look at the structure of the dataset. We shall see what the 2nd image is, and what caption it had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text'],\n",
       "    num_rows: 54949\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=320x50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a sample image from the dataset\n",
    "dataset[2][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H ^ { \\prime } = \\beta N \\int d \\lambda \\biggl \\{ \\frac { 1 } { 2 \\beta ^ { 2 } N ^ { 2 } } \\partial _ { \\lambda } \\zeta ^ { \\dagger } \\partial _ { \\lambda } \\zeta + V ( \\lambda ) \\zeta ^ { \\dagger } \\zeta \\biggr \\} \\ .\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle H ^ { \\prime } = \\beta N \\int d \\lambda \\biggl \\{ \\frac { 1 } { 2 \\beta ^ { 2 } N ^ { 2 } } \\partial _ { \\lambda } \\zeta ^ { \\dagger } \\partial _ { \\lambda } \\zeta + V ( \\lambda ) \\zeta ^ { \\dagger } \\zeta \\biggr \\} \\ .$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render a sample LaTeX text from the dataset\n",
    "from IPython.display import display, Math, Latex\n",
    "latex = dataset[2][\"text\"]\n",
    "print(latex)\n",
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
    "\n",
    "```python\n",
    "[\n",
    "{ \"role\": \"user\",\n",
    "  \"content\": [{\"type\": \"text\",  \"text\": Q}, {\"type\": \"image\", \"image\": image} ]\n",
    "},\n",
    "{ \"role\": \"assistant\",\n",
    "  \"content\": [{\"type\": \"text\",  \"text\": A} ]\n",
    "},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "   return [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        { \"role\" : \"assistant\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"text\"]} ]\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, letâ€™s format the data using the chatbot format for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n",
    "# Test dataset\n",
    "converted_test_dataset = [convert_to_conversation(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at how the conversations are structured for the second example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': [{'type': 'text',\n",
       "   'text': 'Write the LaTeX representation for this image.'},\n",
       "  {'type': 'image',\n",
       "   'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=320x50>}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dataset[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Quantized Model\n",
    "\n",
    "Let's load the quantized model and run inference on the second image from the dataset before we finetune it. Weâ€™ll be using 4 bit GPTQ-Quantized version of Qwen/Qwen2-VL-2B-Instruct. More details on how to quantize a model using GPTQ can be found in my medium article [here](https://medium.com/@arunsreekuttan1996/quantizing-qwen2-vl-models-with-gptqmodel-a-complete-guide-for-multi-modal-model-compression-and-f329ea18a17b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['backend', 'use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                         \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.01124262809753418s                        \n",
      "\u001b[32mINFO\u001b[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, GPTQConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Configure GPTQ settings\n",
    "gptq_config = GPTQConfig(bits=4, use_exllama=False)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=gptq_config\n",
    ")\n",
    "# Set up processor with optimal token ranges\n",
    "min_pixels = 28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512\",\n",
    "    min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you are okay with using a 4bit nf4 (bitsandbytes) quantized model, you can use either Unsloth's fine-tuning notebooks or this notebook with changing the quantization configurations below.\n",
    "\n",
    "```python\n",
    "\n",
    "# Import Libraries\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Configure quantization settings \n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Set up processor with optimal token ranges\n",
    "min_pixels = 28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
    "    min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\\\[\\nH' = \\\\beta N \\\\int d\\\\lambda \\\\left\\\\{ \\\\frac{1}{2\\\\beta^2 N^2} \\\\partial_\\\\lambda \\\\zeta^\\\\dagger \\\\partial_\\\\lambda \\\\zeta + V(\\\\lambda) \\\\zeta^\\\\dagger \\\\right\\\\} .\\n\\\\]\"]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input for the model\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text =  processor.apply_chat_template(messages, add_generation_prompt = True)\n",
    "\n",
    "# Convert the image to the required format\n",
    "image_inputs, _ = process_vision_info(converted_dataset[2])\n",
    "\n",
    "inputs = processor(\n",
    "    text=[input_text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "We will use Huggingface TRL library to finetune our already 4-bit quantized model using GPTQ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure the parameters for LoRA adapter first and check the number of trainable parameters. Note that we are only training the LoRA adapter for language model, and not the vision encoder. This is because, for our purpose the already trained vision encoder is sufficient to extract the features from the images, and we are only finetuning the language model to generate the LaTeX code. Also, my aim is to serve the model using vLLM , which does not support LoRA adapters on vision encoders yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/peft/tuners/lora/layer.py:112: UserWarning: Unsupported layer type '<class 'gptqmodel.nn_modules.qlinear.tritonv2.TritonV2QuantLinear'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 917,197,312 || trainable%: 2.0132\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"down_proj\", \"o_proj\", \"k_proj\", \"q_proj\", \"gate_proj\", \"up_proj\", \"v_proj\"], \n",
    "    init_lora_weights=\"gaussian\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weâ€™ll define the training arguments using the SFTConfig class from the TRL library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments using SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"GPTQ_LoRA\", # Directory to save the model\n",
    "    per_device_train_batch_size = 8, # Batch size for training\n",
    "    gradient_accumulation_steps = 1, # Steps to accumulate gradients\n",
    "    per_device_eval_batch_size = 4, # Batch size for evaluation\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 3, # Number of training epochs\n",
    "    # Optimizer and scheduler settings\n",
    "    learning_rate = 2e-4, # Learning rate for training\n",
    "    optim = \"adamw_torch_fused\", # Optimizer type\n",
    "    weight_decay = 0.01, # Weight decay for regularization\n",
    "    lr_scheduler_type = \"linear\", # Type of learning rate scheduler\n",
    "    # Mixed precision and gradient settings\n",
    "    fp16 = False,\n",
    "    bf16 = True,\n",
    "    # Logging and evaluation\n",
    "    logging_steps = 1,\n",
    "    seed = 3407,\n",
    "    report_to = \"none\",     # For Weights and Biases\n",
    "    save_strategy=\"epoch\",  # We will save the model at the end of each epoch\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "\n",
    "    # You MUST put the below items for vision finetuning:\n",
    "    remove_unused_columns = False,\n",
    "    dataset_text_field = \"\",\n",
    "    dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "    dataset_num_proc = 4,\n",
    "    max_seq_length = 2048,\n",
    "    label_names = [\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a collator function to properly retrieve and batch the data during the training procedure. This function will handle the formatting of our dataset inputs, ensuring they are correctly structured for the model. Letâ€™s define the collator function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLProcessor\n",
    "\n",
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)  # Encode texts and images into tensors\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define the SFTTrainer, which is a wrapper around the transformers.Trainer class and inherits its attributes and methods. This class simplifies the fine-tuning process by properly initializing the PeftModel when a PeftConfig object is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor.tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=converted_dataset,\n",
    "    eval_dataset=converted_test_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model. It approximately took 2 hours to train on a single A100 GPU for 3 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20607' max='20607' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20607/20607 1:53:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.097750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.088671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.097618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20607, training_loss=0.07141266917244371, metrics={'train_runtime': 6808.4222, 'train_samples_per_second': 24.212, 'train_steps_per_second': 3.027, 'total_flos': 1.2934115043647078e+17, 'train_loss': 0.07141266917244371})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Finetuned model\n",
    "\n",
    "Restart the kernel and run the inference on the finetuned model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the baseline model following the same pipeline as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['backend', 'use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.006639242172241211s                       \n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, GPTQConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Configure GPTQ settings - crucial for T4 GPUs\n",
    "gptq_config = GPTQConfig(bits=4, use_exllama=False)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=gptq_config\n",
    ")\n",
    "# Set up processor with optimal token ranges\n",
    "min_pixels = 28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512\",\n",
    "    min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a sample from the test set to evaluate our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( 5 . 1 7 ) = - \\\\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\\\left\\\\{ \\\\frac { 1 } { 4 } \\\\frac { 1 } { \\\\lambda } z ^ { 2 \\\\lambda } + \\\\frac { 1 } { 2 } \\\\ln ( z ) \\\\right\\\\}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "index = 25\n",
    "latex = test_dataset[index][\"text\"]\n",
    "latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=320x50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[index][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's render the ground truth equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle ( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\left\\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 \\lambda } + \\frac { 1 } { 2 } \\ln ( z ) \\right\\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write just the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text =  processor.apply_chat_template(messages, add_generation_prompt = True)\n",
    "image_inputs, _ = process_vision_info(converted_test_dataset[index])\n",
    "inputs = processor(\n",
    "    text=[input_text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/peft/tuners/lora/layer.py:112: UserWarning: Unsupported layer type '<class 'gptqmodel.nn_modules.qlinear.tritonv2.TritonV2QuantLinear'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n",
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adapter_path_epoch_1 = \"/home/madhusudhanan.a/vlms/GPTQ_LoRA/checkpoint-6869\"\n",
    "adapter_path_epoch_2 = \"/home/madhusudhanan.a/vlms/GPTQ_LoRA/checkpoint-13738\"\n",
    "adapter_path_epoch_3 = \"/home/madhusudhanan.a/vlms/GPTQ_LoRA/checkpoint-20607\"\n",
    "model.load_adapter(adapter_path_epoch_1,adapter_name=\"epoch_1\")\n",
    "model.load_adapter(adapter_path_epoch_2,adapter_name=\"epoch_2\")\n",
    "model.load_adapter(adapter_path_epoch_3,adapter_name=\"epoch_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the inference using baseline quantized model. We will use `model.disable_adapters_adapters()` to disable lora adapters. Make sure to enable adapters back using `model.enable_adapters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\begin{equation}\\n\\\\frac{1}{m+n-1}z^{m+n-1} = \\\\frac{1}{4\\\\lambda}z^{2\\\\lambda} + \\\\frac{1}{\\\\ln(z)}\\n\\\\end{equation}']\n"
     ]
    }
   ],
   "source": [
    "model.disable_adapters() # Disable adapters\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "model.enable_adapters() # Enable adapters back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's render the output from baseline quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{equation}\n",
       "\\frac{1}{m+n-1}z^{m+n-1} = \\frac{1}{4\\lambda}z^{2\\lambda} + \\frac{1}{\\ln(z)}\n",
       "\\end{equation}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(output_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the baseline quantized model does not perform well in predicting the correct latex equation.\n",
    "\n",
    "Now let's set the LoRA adapter from epoch 3 as the active adapter and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['( 5 . 1 7 ) = - \\\\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\\\{ \\\\frac { 1 } { 4 } \\\\frac { 1 } { \\\\lambda } z ^ { 2 \\\\lambda } + \\\\frac { 1 } { 2 } \\\\ln ( z ) \\\\}']\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle ( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 \\lambda } + \\frac { 1 } { 2 } \\ln ( z ) \\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.set_adapter(\"epoch_3\")  # Set the adapter to the last epoch\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "display(Math(output_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model performs well on the test sample !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Let's evaluate the model on test dataset to see how well our model performs on the test set. We will evaluate our model using the following Metric:\n",
    "1. [Bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
    "2. [Levenshtein edit distance](https://medium.com/@ethannam/understanding-the-levenshtein-distance-equation-for-beginners-c4285a5604f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary whitespace from LaTeX code.\n",
    "import re\n",
    "def post_process(s: str):\n",
    "    text_reg = r'(\\\\(operatorname|mathrm|text|mathbf)\\s?\\*? {.*?})'\n",
    "    letter = '[a-zA-Z]'\n",
    "    noletter = '[\\W_^\\d]'\n",
    "    names = [x[0].replace(' ', '') for x in re.findall(text_reg, s)]\n",
    "    s = re.sub(text_reg, lambda match: str(names.pop(0)), s)\n",
    "    news = s\n",
    "    while True:\n",
    "        s = news\n",
    "        news = re.sub(r'(?!\\\\ )(%s)\\s+?(%s)' % (noletter, noletter), r'\\1\\2', s)\n",
    "        news = re.sub(r'(?!\\\\ )(%s)\\s+?(%s)' % (noletter, letter), r'\\1\\2', news)\n",
    "        news = re.sub(r'(%s)\\s+?(%s)' % (letter, noletter), r'\\1\\2', news)\n",
    "        if news == s:\n",
    "            break\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate Baseline model, disable adapter first (Uncomment the line below)\n",
    "# model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [03:43<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.832773655559199\n",
      "Average Normalized Edit Distance: 0.0922510220772206\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# Initialize evaluation tools\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "norm_edit_dists = []\n",
    "\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "batch_size = 64\n",
    "n_samples = len(converted_test_dataset)\n",
    "\n",
    "for start_idx in tqdm(range(0, n_samples, batch_size), desc=\"Evaluating\"):\n",
    "    batch = converted_test_dataset[start_idx : start_idx + batch_size]\n",
    "\n",
    "    batch_texts = []\n",
    "    batch_images = []\n",
    "    batch_ground_truths = []\n",
    "\n",
    "    for sample in batch:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": instruction}\n",
    "            ]}\n",
    "        ]\n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        batch_texts.append(input_text)\n",
    "\n",
    "        image_inputs, _ = process_vision_info(sample)\n",
    "        batch_images.append(image_inputs)\n",
    "\n",
    "        gt = sample[1]['content'][0]['text']\n",
    "        batch_ground_truths.append(gt)\n",
    "\n",
    "    # Process batch\n",
    "    inputs = processor(\n",
    "        text=batch_texts,\n",
    "        images=batch_images,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    batch_outputs = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    for pred, true in zip(batch_outputs, batch_ground_truths):\n",
    "        all_predictions.append(pred)\n",
    "        all_references.append([true])  # wrap for BLEU\n",
    "\n",
    "        norm_dist = Levenshtein.distance(post_process(pred), post_process(true)) / max(len(pred), len(true))\n",
    "        norm_edit_dists.append(norm_dist)\n",
    "    \n",
    "# Final evaluation\n",
    "bleu_result = bleu_metric.compute(predictions=all_predictions, references=all_references)\n",
    "average_edit_distance = sum(norm_edit_dists) / len(norm_edit_dists)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"BLEU Score:\", bleu_result['bleu'])\n",
    "print(\"Average Normalized Edit Distance:\", average_edit_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub\n",
    "\n",
    "Finally, we can push the model to the Hugging Face Hub. This will allow others to use our finetuned model easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73.9M/73.9M [00:01<00:00, 50.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_qptq_epoch1/commit/976b7bec87b7cb988436bbbf7b4870354254eb13', commit_message='Upload Qwen2VLForConditionalGeneration', commit_description='', oid='976b7bec87b7cb988436bbbf7b4870354254eb13', pr_url=None, repo_url=RepoUrl('https://huggingface.co/arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_qptq_epoch1', endpoint='https://huggingface.co', repo_type='model', repo_id='arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_qptq_epoch1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_qptq_epoch3\", token = \"ur_token_here\") # Online saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
