{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use the VLLM library for efficient inference using LoRA (Low-Rank Adaptation) adapters on the top of base models. The example uses two variants of Qwen2-VL-2B models finetuned on LaTeX-OCR dataset:\n",
    "- 4-bit nf4(bits and bytes) quantized model + LoRA (aka QLoRA)\n",
    "- 4-bit GPTQ Quantized model + LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Inference\n",
    "\n",
    "We can run vLLM in offline mode to be used in our local projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-20 21:43:26 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:43:30,042\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import gc\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import LLM, EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from dataclasses import asdict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the EngineArgs for the LLMEngine\n",
    "def initialize_engine(model: str, quantization: str,modality: str) -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine.\"\"\"    \n",
    "    if quantization == \"bitsandbytes\":\n",
    "        engine_args = EngineArgs(\n",
    "            model=model, # Specify the model name or path\n",
    "            max_model_len=4096, # Model context length. If unspecified, will be automatically derived from the model config.\n",
    "            max_num_seqs=5, # Maximum number of sequences per iteration.\n",
    "            enable_lora=True, # If True, enable handling of LoRA adapters.\n",
    "            max_loras=1, # Max number of LoRAs in a single batch.\n",
    "            max_lora_rank=16, # Max LoRA rank.\n",
    "            mm_processor_kwargs={ # Specify the multimodal processor kwargs\n",
    "                \"min_pixels\": 28 * 28,\n",
    "                \"max_pixels\": 1280 * 28 * 28,\n",
    "            },\n",
    "            limit_mm_per_prompt={modality: 1}, # Limit the number of multimodal inputs per prompt.\n",
    "            quantization=quantization, # Specify the quantization method (E.g., \"bitsandbytes\", \"gptq\" etc.)\n",
    "            load_format=\"bitsandbytes\", # Load format for the model. Need to specify this if using bitsandbytes quantization.\n",
    "            max_seq_len_to_capture = 4096,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        engine_args = EngineArgs(\n",
    "            model=model,\n",
    "            max_model_len=4096,\n",
    "            max_num_seqs=5,\n",
    "            enable_lora=True,\n",
    "            max_loras=3,\n",
    "            max_lora_rank=16,\n",
    "            mm_processor_kwargs={\n",
    "                \"min_pixels\": 28 * 28,\n",
    "                \"max_pixels\": 1280 * 28 * 28,\n",
    "            },\n",
    "            limit_mm_per_prompt={modality: 1},\n",
    "            quantization=quantization,\n",
    "            max_seq_len_to_capture = 4096,\n",
    "        )\n",
    "    \n",
    "    # Set number of multimodal inputs other than the specified modality to 0\n",
    "    default_limits = {\"image\": 0, \"video\": 0, \"audio\": 0}\n",
    "    engine_args.limit_mm_per_prompt = default_limits | dict(engine_args.limit_mm_per_prompt or {}) \n",
    "\n",
    "    return engine_args\n",
    "\n",
    "\n",
    "# function to generate prompts\n",
    "def generate_prompts(questions: list[str], modality: str) -> list[str]:\n",
    "    if modality == \"image\":\n",
    "        placeholder = \"<|image_pad|>\"\n",
    "    elif modality == \"video\":\n",
    "        placeholder = \"<|video_pad|>\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported modality: {modality}\")\n",
    "   \n",
    "    # Create prompts with the specified placeholder for the modality\n",
    "    # The prompt format should follow corresponding examples on HuggingFace model repository.\n",
    "    prompts = [\n",
    "        (\n",
    "            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n",
    "            f\"{question}<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        for question in questions\n",
    "    ]\n",
    "\n",
    "    return prompts  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to download the adapter(s) and save them locally using the snapshot_download function\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "modality = \"image\"\n",
    "quantization = \"gptq_marlin\" # Options for quantization: \"bitsandbytes\", \"gptq_marlin\", \"gptq\"\n",
    "\n",
    "\n",
    "# GPTQ Marlin is a more efficient kernel for running GPTQ quantized models.\n",
    "# if you your are using NVIDIA Turing or older GPUs (e.g., T4, V100, P100), you need to use just \"gptq\" as the quantization method.\n",
    "if quantization == \"gptq_marlin\":\n",
    "    model_name = \"arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512\"\n",
    "    vision_lora_path = snapshot_download(repo_id=\"arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_qptq_epoch3\")\n",
    "else:\n",
    "    model_name = \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\"\n",
    "    vision_lora_path = snapshot_download(repo_id=\"arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_nf4_epoch3\")\n",
    "\n",
    "# Let's create the vLLM LLM engine with the specified model, quantization, and engine configurations\n",
    "engine_args = initialize_engine(model_name, quantization, modality)\n",
    "engine_args = asdict(engine_args)\n",
    "llm = LLM(**engine_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 237.34it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 79.80 toks/s, output: 159.59 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'( 5 . 1 7 ) = - \\\\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\\\left\\\\{ \\\\frac { 1 } { 4 } \\\\frac { 1 } { \\\\lambda } z ^ { 2 \\\\lambda } + \\\\frac { 1 } { 2 } \\\\ln ( z ) \\\\right\\\\}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the input for the LLM\n",
    "data = Image.open(\"/home/madhusudhanan.a/vlms/latex.png\")\n",
    "questions = [\"Write the LaTeX representation for this image.\"]\n",
    "prompts = generate_prompts(questions, modality)\n",
    "inputs = {\n",
    "    \"prompt\": prompts[0],\n",
    "    \"multi_modal_data\": {modality: data},    \n",
    "}\n",
    "\n",
    "# Define the sampling parameters \n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    stop_token_ids=[151645]\n",
    ")\n",
    "\n",
    "# Define the LoRA request for the vision model\n",
    "# The first parameter of LoRARequest is a human identifiable name, the second parameter is a globally unique ID for the adapter and the third parameter is the path to the LoRA adapter.\n",
    "lora_request=LoRARequest(\"vision\", 1, vision_lora_path)\n",
    "\n",
    "\n",
    "# Generate the output using the LLM engine\n",
    "outputs = llm.generate(\n",
    "    inputs,\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=lora_request,\n",
    ")\n",
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle ( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\left\\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 \\lambda } + \\frac { 1 } { 2 } \\ln ( z ) \\right\\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(outputs[0].outputs[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Inference\n",
    "\n",
    "vLLM provides an HTTP server that implements OpenAI's Completions API, Chat API, and more! This functionality lets us serve models and interact with them using an HTTP client. We need to start the server first with `vllm serve` command.\n",
    "\n",
    "The same configuration we used for offline inference can be used for online inference. Open a terminal and run the following commands\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the GPTQ Quantized model + LoRA adapter using vLLM, you can use the following command. Note that for Turing or older GPUs, you need to use the \"gptq\" quantization method instead of \"gptq_marlin\" and set \"dtype\" to float16.\n",
    "\n",
    "``` bash\n",
    "vllm serve arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512 \\\n",
    "    --enable-lora \\\n",
    "    --lora-modules '{\"name\": \"vision\", \"path\": \"arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_qptq_epoch3\", \"base_model_name\": \"arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512\"}'\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 4096 \\\n",
    "    --max-num-seqs 5 \\\n",
    "    --max-loras 1 \\\n",
    "    --max-lora-rank 16 \\\n",
    "    --quantization gptq_marlin \\\n",
    "    --limit-mm-per-prompt \"image=1,video=0\" \\\n",
    "    --max-seq-len-to-capture 4096 \\\n",
    "    --mm-processor-kwargs '{\"min_pixels\": 784, \"max_pixels\": 1003520}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the QLoRA model(nf4 quantization + LoRA adapter) using vLLM, you can use the following command. Note that for Turing or older GPUs, you need to set the \"dtype\" to float16.4\n",
    "\n",
    "``` bash\n",
    "vllm serve unsloth/Qwen2-VL-2B-Instruct-bnb-4bit \\\n",
    "    --enable-lora \\\n",
    "    --lora-modules '{\"name\": \"vision\", \"path\": \"arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_nf4_epoch3\", \"base_model_name\": \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\"}'\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 4096 \\\n",
    "    --max-num-seqs 5 \\\n",
    "    --max-loras 1 \\\n",
    "    --max-lora-rank 16 \\\n",
    "    --quantization bitsandbytes \\\n",
    "    --load-format bitsandbytes \\\n",
    "    --limit-mm-per-prompt \"image=1,video=0\" \\\n",
    "    --max-seq-len-to-capture 4096 \\\n",
    "    --mm-processor-kwargs '{\"min_pixels\": 784, \"max_pixels\": 1003520}'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the server, in your preferred text editor, create a script that uses an HTTP client. Include any messages that you want to send to the model. Then run that script. Below is an example script :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, you can set the NO_PROXY environment variable to avoid proxy issues when running the vLLM server locally.\n",
    "import os\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "os.environ[\"no_proxy\"] = \"localhost,127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\left\\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 \\lambda } + \\frac { 1 } { 2 } \\ln ( z ) \\right\\}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "#  Encode a local image file to base64 format.\n",
    "def encode_base64_image(image_path: str) -> str:\n",
    "   \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Configure the OpenAI client to point to your local vLLM server\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",  # vLLM does not require an API key by default\n",
    "    base_url=\"http://localhost:8000/v1\"\n",
    ")\n",
    "\n",
    "# Path to your local image\n",
    "image_path = \"/home/madhusudhanan.a/vlms/latex.png\"\n",
    "\n",
    "# Encode the image to base64\n",
    "image_base64 = encode_base64_image(image_path)\n",
    "\n",
    "# Create the chat completion request with the image\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"vision\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Write the LaTeX representation for this image.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    "    max_tokens=128,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle ( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\left\\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 \\lambda } + \\frac { 1 } { 2 } \\ln ( z ) \\right\\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(chat_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\left\\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 } \\lambda ^ { 2 } + \\frac { 1 } { 2 } \\ln ( z ) \\right\\}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "#  Encode a local image file to base64 format.\n",
    "def encode_base64_image(image_path: str) -> str:\n",
    "   \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Configure the OpenAI client to point to your local vLLM server\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",  # vLLM does not require an API key by default\n",
    "    base_url=\"http://localhost:8000/v1\"\n",
    ")\n",
    "\n",
    "# Path to your local image\n",
    "image_path = \"/home/madhusudhanan.a/vlms/latex.png\"\n",
    "\n",
    "# Encode the image to base64\n",
    "image_base64 = encode_base64_image(image_path)\n",
    "\n",
    "# Create the chat completion request with the image\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"vision\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Write the LaTeX representation for this image.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    "    max_tokens=128,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle ( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\left\\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 } \\lambda ^ { 2 } + \\frac { 1 } { 2 } \\ln ( z ) \\right\\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(chat_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking our model\n",
    "\n",
    "Benchmarking is an important step to evaluate the performance of our model and to ensure that it meets the throughput requirements for our specific use case and adjust the configuration accordingly. vLLM provides scripts to benchmark the model performance while serving the model. The script currently supports running benchmarks using certain number of datasets such as ShareGPT, BurstGPT, VisionArena, and more.\n",
    "\n",
    "However the benchmarking script currently does not support the Latex_OCR dataset, which is used for finetuning the models in this example.\n",
    "\n",
    "I have updated the script to support Latex_OCR dataset and raised a PR ([#19894](https://github.com/vllm-project/vllm/pull/19894)) to vLLM repository. \n",
    "As of now PR is not merged yet, so you can use the updated script from my PR branch [here](https://github.com/arunmadhusud/vllm/tree/unsloth_benchmark). You can either build vLLM from my forked repository or replace the `benchmark_datset.py` and `benchmark_serving.py` files in the your vLLM installation with the updated files from my PR branch. I would recommend doing the latter as it is easier and faster.\n",
    "\n",
    "Now start the vLLM server with the vLLM serve command as shown above, and then run the benchmark script with the following command. \n",
    "\n",
    "We will use `openai-chat` as the backend from the available options:\n",
    "{tgi, vllm, lmdeploy, deepspeed-mii, openai, openai-chat, openai-audio, tensorrt-llm, scalellm, sglang}.\n",
    "\n",
    "We will set  `request-rate` ato 5, which means 5 requests will be sent to the model per second. We will also set `max-concurrency` to 5, meaning up to 5 requests can be processed simultaneously. While the `request-rate` argument controls the rate at which requests are initiated, `max-concurrency` will control how many are actually allowed to execute at a time. This means that when used in combination, the actual request rate (throughput) may be lower than specified with `request-rate`, if the server is not processing requests fast enough to keep up.\n",
    "\n",
    "Other arguments used:\n",
    "- `--model`: The model to benchmark\n",
    "- `--dataset-name`: The dataset type to benchmark, ours is `hf` (hugging face dataset)\n",
    "- `--dataset-path`: The name of the Hugging Face dataset. Ours is `unsloth/latex_ocr`\n",
    "- `--hf-split`: The dataset split to use (train/test dataset), We use `train`\n",
    "- `--hf-output-len`: The maximum number of output tokens to generate, set to 256\n",
    "- `--num-prompts`: The number of prompts to use for benchmarking. We use 1000 samples.\n",
    "If the dataset has fewer than 1000 samples, it will be re-sampled to reach 1000.\n",
    "- `--lora_modules`: Name of the LoRA adapter(s) to use. Ours is `vision`. This should match the name used when serving the model.\n",
    "\n",
    "We will use the default percentile metrics: `TTFT`, `TPOT`, and `ITL`. \n",
    "- Time To First Token (`TTFT`) is the time taken to generate the first token.\n",
    "- Time per Output Token (`TPOT`) is the time taken to generate each token (exluding the first token)\n",
    "- Inter-Token Latency (`ITL`) is the time taken between each token generation. \n",
    "\n",
    "The mean, median and 99th percentile values for these metrics will be reported in the output.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-20 21:21:03 [__init__.py:244] Automatically detected platform cuda.\n",
      "Namespace(backend='openai-chat', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/chat/completions', dataset_name='hf', dataset_path='unsloth/LaTeX_OCR', max_concurrency=5, model='arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512', tokenizer=None, use_beam_search=False, num_prompts=1000, logprobs=None, request_rate=5.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split='train', hf_output_len=256, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=['vision'])\n",
      "Starting initial single prompt test run...\n",
      "Initial test run completed. Starting main benchmark run...\n",
      "Traffic request rate: 5.0\n",
      "Burstiness factor: 1.0 (Poisson process)\n",
      "Maximum request concurrency: 5\n",
      "100%|███████████████████████████████████████| 1000/1000 [03:21<00:00,  4.96it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     1000      \n",
      "Benchmark duration (s):                  201.52    \n",
      "Total input tokens:                      8000      \n",
      "Total generated tokens:                  70865     \n",
      "Request throughput (req/s):              4.96      \n",
      "Output token throughput (tok/s):         351.64    \n",
      "Total Token throughput (tok/s):          391.34    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          62.75     \n",
      "Median TTFT (ms):                        56.72     \n",
      "P99 TTFT (ms):                           102.23    \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          7.38      \n",
      "Median TPOT (ms):                        7.35      \n",
      "P99 TPOT (ms):                           9.97      \n",
      "---------------Inter-token Latency----------------\n",
      "Mean ITL (ms):                           7.26      \n",
      "Median ITL (ms):                         5.99      \n",
      "P99 ITL (ms):                            40.52     \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# To benchmark the serving performance of GPTQ Quantized model + LoRA adapter using vLLM, you can use the following command. \n",
    "'''\n",
    "backend options'''\n",
    "!python3 vllm/benchmarks/benchmark_serving.py \\\n",
    "  --backend openai-chat \\\n",
    "  --request-rate 5 \\\n",
    "  --max-concurrency 5 \\\n",
    "  --model arunmadhusudh/Qwen2-VL-2B-Instruct-4bit-GPTQ_T4_tr4512 \\\n",
    "  --endpoint /v1/chat/completions \\\n",
    "  --dataset-name hf \\\n",
    "  --dataset-path unsloth/LaTeX_OCR \\\n",
    "  --hf-split train \\\n",
    "  --hf-output-len 256 \\\n",
    "  --num-prompts 1000 \\\n",
    "  --lora_modules vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-20 21:34:28 [__init__.py:244] Automatically detected platform cuda.\n",
      "Namespace(backend='openai-chat', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/chat/completions', dataset_name='hf', dataset_path='unsloth/LaTeX_OCR', max_concurrency=5, model='unsloth/Qwen2-VL-2B-Instruct-bnb-4bit', tokenizer=None, use_beam_search=False, num_prompts=1000, logprobs=None, request_rate=5.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split='train', hf_output_len=256, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=['vision'])\n",
      "Starting initial single prompt test run...\n",
      "Initial test run completed. Starting main benchmark run...\n",
      "Traffic request rate: 5.0\n",
      "Burstiness factor: 1.0 (Poisson process)\n",
      "Maximum request concurrency: 5\n",
      "100%|███████████████████████████████████████| 1000/1000 [04:37<00:00,  3.60it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     1000      \n",
      "Benchmark duration (s):                  277.65    \n",
      "Total input tokens:                      8000      \n",
      "Total generated tokens:                  70314     \n",
      "Request throughput (req/s):              3.60      \n",
      "Output token throughput (tok/s):         253.25    \n",
      "Total Token throughput (tok/s):          282.06    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          96.62     \n",
      "Median TTFT (ms):                        90.32     \n",
      "P99 TTFT (ms):                           157.86    \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          18.57     \n",
      "Median TPOT (ms):                        18.50     \n",
      "P99 TPOT (ms):                           22.52     \n",
      "---------------Inter-token Latency----------------\n",
      "Mean ITL (ms):                           18.32     \n",
      "Median ITL (ms):                         15.09     \n",
      "P99 ITL (ms):                            76.95     \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# To benchmark the serving performance of QLoRA model(nf4 quantization + LoRA adapter) using vLLM, you can use the following command.\n",
    "!python3 vllm/benchmarks/benchmark_serving.py \\\n",
    "  --backend openai-chat \\\n",
    "  --request-rate 5 \\\n",
    "  --max-concurrency 5 \\\n",
    "  --model unsloth/Qwen2-VL-2B-Instruct-bnb-4bit \\\n",
    "  --endpoint /v1/chat/completions \\\n",
    "  --dataset-name hf \\\n",
    "  --dataset-path unsloth/LaTeX_OCR \\\n",
    "  --hf-split train \\\n",
    "  --hf-output-len 256 \\\n",
    "  --num-prompts 1000 \\\n",
    "  --lora_modules vision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
