{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to fine-tune a 4-bit quantized model (quantized using bitsandbytes) with the QLoRA technique, utilizing the Unsloth library. It is adapted from the official Unsloth fine-tuning notebook with the following modifications:\n",
    "- LoRA configuration has been adjusted to exclude vision layers\n",
    "- An evaluation script has been added to assess the fine-tuned mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset,test_dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=['train[:80%]', 'test[:20%]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    conversation = [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        { \"role\" : \"assistant\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"text\"]} ]\n",
    "        },\n",
    "    ]\n",
    "    return { \"messages\" : conversation }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n",
    "converted_test_dataset = [convert_to_conversation(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Baseline Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-20 23:18:19 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 23:18:23,453\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Qwen2 patching. Transformers: 4.52.3. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.15 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    target_modules=[\"down_proj\", \"o_proj\", \"k_proj\", \"q_proj\", \"gate_proj\", \"up_proj\", \"v_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\\\begin{align*} H' &= \\\\beta N \\\\int d\\\\lambda \\\\left\\\\{ \\\\frac{1}{2\\\\beta^2 N^2} \\\\partial_\\\\lambda \\\\zeta^\\\\dagger \\\\partial_\\\\lambda \\\\zeta + V(\\\\lambda) \\\\zeta^\\\\dagger \\\\right\\\\} \\\\end{align*}\"]\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = dataset[2][\"image\"]\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Run inference without streamer\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1\n",
    ")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = tokenizer.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_test_dataset,  # Optional, but useful\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        per_device_eval_batch_size = 4,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 30,\n",
    "        num_train_epochs = 3, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "        save_strategy=\"epoch\",  # Strategy for saving the model\n",
    "        eval_strategy=\"epoch\",\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.15 GB.\n",
      "2.438 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 54,949 | Num Epochs = 3 | Total steps = 20,607\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 18,464,768/2,000,000,000 (0.92% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20607' max='20607' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20607/20607 2:51:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.100811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.090811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.096599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Configure quantization settings \n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Set up processor with optimal token ranges\n",
    "min_pixels = 28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
    "    min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( 5 . 1 7 ) = - \\\\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\\\left\\\\{ \\\\frac { 1 } { 4 } \\\\frac { 1 } { \\\\lambda } z ^ { 2 \\\\lambda } + \\\\frac { 1 } { 2 } \\\\ln ( z ) \\\\right\\\\}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "index = 25\n",
    "latex = test_dataset[index][\"text\"]\n",
    "latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=320x50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[index][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "   return [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        { \"role\" : \"assistant\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"text\"]} ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n",
    "converted_test_dataset = [convert_to_conversation(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write just the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text =  processor.apply_chat_template(messages, add_generation_prompt = True)\n",
    "image_inputs, _ = process_vision_info(converted_test_dataset[index])\n",
    "inputs = processor(\n",
    "    text=[input_text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhusudhanan.a/.conda/envs/tensorrt_env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adapter_path_epoch_1 = \"/home/madhusudhanan.a/vlms/outputs/checkpoint-6869\"\n",
    "adapter_path_epoch_2 = \"/home/madhusudhanan.a/vlms/outputs/checkpoint-13738\"\n",
    "adapter_path_epoch_3 = \"/home/madhusudhanan.a/vlms/outputs/checkpoint-20607\"\n",
    "model.load_adapter(adapter_path_epoch_1,adapter_name=\"epoch_1\")\n",
    "model.load_adapter(adapter_path_epoch_2,adapter_name=\"epoch_2\")\n",
    "model.load_adapter(adapter_path_epoch_3,adapter_name=\"epoch_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the inference using baseline quantized model. We will use `model.disable_adapters_adapters()` to disable lora adapters. Make sure to enable adapters back using `model.enable_adapters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\begin{equation*}\\nZ = \\\\frac{H}{N} + N^2 + H^2 = \\\\frac{24}{\\\\lambda \\\\kappa}\\n\\\\end{equation*}', '\\\\[\\n\\\\tilde{Z}_0^{-1} = \\\\int \\\\mathcal{D} \\\\varphi \\\\mathcal{D} \\\\varphi^* \\\\exp \\\\left\\\\{ \\\\frac{i}{m} \\\\int d^4 x \\\\varphi^* \\\\left( \\\\partial^\\\\mu \\\\partial_\\\\mu + m^2 \\\\right) \\\\varphi \\\\right\\\\}.\\n\\\\]', '\\\\begin{align*}\\n\\\\varphi(x^+, x^-, +I, x^1 + sI, x^2) = \\\\varphi(x^+, x^-, x^1, x^2),\\n\\\\end{align*}', '(12,12),(987,987)', '(123,131),(877,937)', '\\\\begin{align*}\\n\\\\left[ \\\\sigma_c \\\\sigma_c \\\\right] \\\\epsilon_c X^u = (a^u \\\\epsilon^u + a^c \\\\epsilon^c)\\n\\\\end{align*}', '\\\\begin{equation*}\\n\\\\operatorname{tr}(R \\\\cap \\\\dots \\\\cap R) = \\\\operatorname{tr}(R^k).\\n\\\\end{equation*}', 'The LaTeX representation for the image is:\\n\\n```latex\\nN = tr \\\\int d^3 x \\\\left( A_\\\\mu \\\\frac{\\\\delta}{\\\\delta A_\\\\mu} + c \\\\frac{\\\\delta}{\\\\delta c} + A_\\\\mu \\\\frac{\\\\delta}{\\\\delta A_\\\\mu^*} + c^* \\\\frac{\\\\delta}{\\\\delta c^*} \\\\right).\\n```', '\\\\[\\n\\\\frac{1}{\\\\sqrt{-\\\\eta \\\\eta^{\\\\mu \\\\nu}}} = \\\\frac{1}{1 - \\\\lambda^2} \\\\left( \\\\lambda_{-} - \\\\frac{1 + \\\\lambda^2}{\\\\lambda_{-+}} \\\\right)\\n\\\\]', '\\\\begin{equation*}\\n\\\\delta \\\\left( t_{2} | t_{2} \\\\right) = i \\\\left\\\\{ \\\\left[ \\\\delta S_{+} \\\\Phi_{+} + J_{+} \\\\Phi_{+} - S_{-} \\\\left[ \\\\Phi_{-} \\\\right] - J \\\\left[ \\\\Phi_{-} \\\\right] \\\\right] \\\\right\\\\},\\n\\\\end{equation*}', '\\\\begin{equation*}\\nS_1 = - \\\\int d^5 \\\\sigma \\\\sqrt{-\\\\det (G_{\\\\mu \\\\nu} + F_{\\\\mu \\\\nu})} + \\\\int ( \\\\mathcal{H} \\\\wedge \\\\mathcal{F} - \\\\frac{1}{2} C_1 \\\\wedge \\\\mathcal{F} \\\\wedge \\\\mathcal{F}),\\n\\\\end{equation*}', '\\\\[\\np_a^{\\\\theta} = -i \\\\partial_a.\\n\\\\]', '\\\\[\\na \\\\rightarrow a_D a_D \\\\rightarrow \\\\rightarrow a.\\n\\\\]', '\\\\begin{align*}\\n\\\\{M_a, M_b\\\\}_k &= 4V \\\\delta_{ab} \\\\\\\\\\n\\\\{M_a, M_b\\\\}_0 &= 0.\\n\\\\end{align*}', '\\\\[\\n\\\\begin{aligned}\\nR_B^A &= \\\\left[ R_b^a - e_l^{-2} e_b^a \\\\; e_l^{-1} T_b^a \\\\right] \\\\\\\\\\n&= 0\\n\\\\end{aligned}\\n\\\\]', '\\\\begin{equation*}\\n\\\\frac{1}{\\\\theta} \\\\left( \\\\frac{\\\\partial^2 \\\\theta}{\\\\partial \\\\theta^2} + \\\\frac{\\\\cos \\\\theta}{\\\\sin \\\\theta} \\\\frac{\\\\partial \\\\theta}{\\\\partial \\\\theta} \\\\right) + \\\\frac{1}{\\\\sin^2 \\\\theta} \\\\frac{1}{\\\\theta} \\\\frac{\\\\partial^2 \\\\theta}{\\\\partial \\\\theta^2} \\\\equiv - (l + 1),\\n\\\\end{equation*}', '```latex\\n\\\\frac{\\\\partial}{\\\\partial \\\\theta} W = i \\\\hbar \\\\Delta W\\n```', '\\\\begin{align*}\\n\\\\frac{1}{2}\\\\rho\\\\kappa^{-2} = Y^2 + U + V - X^2 - Z \\\\geq 0.\\n\\\\end{align*}', 'The LaTeX representation for the image is:\\n\\n```latex\\nds^2 = \\\\frac{1}{z^2} \\\\left( dz^2 + dz^* dz \\\\right)\\n```', '\\\\begin{equation}\\nf_{p,q}(t) = f(t, \\\\Delta_{p,q}) - \\\\beta_{p,q} - f_{p-1,q}(t) - f_{p,q-1}(t) - f_{p-1,q-1}(t).\\n\\\\end{equation}', '\\\\begin{equation}\\nM_1 + M_n M_{n+1} + \\\\dots + M_{n+2} = I\\n\\\\end{equation}', '\\\\[\\n\\\\ln \\\\frac{1}{2} = \\\\frac{1}{2} \\\\cdot g^2 \\\\left( T^m \\\\right) \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\mu \\\\mu \\\\delta \\\\', '\\\\begin{equation*}\\n\\\\hat{E} = \\\\int_{\\\\mathcal{G}} \\\\hat{t}_{\\\\mu\\\\nu} \\\\hat{v} \\\\, d\\\\sigma^{\\\\mu\\\\nu} = \\\\hat{N} - \\\\hat{T}_{\\\\mu\\\\nu} \\\\hat{Q}.\\n\\\\end{equation*}', '```latex\\nI^{ren}(p) = \\\\exp \\\\left[ \\\\frac{1}{\\\\epsilon} \\\\int_0^1 dt \\\\frac{dt}{2t} (\\\\sqrt{1 - 4t \\\\lambda} - 1) \\\\right] I(p)\\n```', '\\\\begin{align*}\\n\\\\frac{1}{2Q \\\\cdot k_i + k_i^2} &= \\\\frac{1}{2Q \\\\cdot k_i} - \\\\frac{k_i^2}{(2Q \\\\cdot k_i)^2} + \\\\frac{(k_i^2)^2}{(2Q \\\\cdot k_i)^3} + \\\\cdots\\n\\\\end{align*}', '\\\\[\\nx \\\\rightarrow x, \\\\; y \\\\rightarrow \\\\alpha y, \\\\; z \\\\rightarrow \\\\alpha^2 z\\n\\\\]', '\\\\begin{equation*}\\n\\\\operatorname{tr}(F \\\\wedge F) = d \\\\omega _ {3} (A) \\\\text{ and } \\\\delta \\\\xi \\\\omega _ {3} (A) = \\\\operatorname{dtr} (\\\\xi \\\\, F) \\\\; ,\\n\\\\end{equation*}', '\\\\begin{equation}\\n(\\\\theta_t + \\\\vartheta_x v) (\\\\theta_t + v \\\\vartheta_x) \\\\phi = \\\\theta_t^2 \\\\phi - \\\\frac{1}{k_0^2 \\\\theta_t^2} \\\\phi\\n\\\\end{equation}', '\\\\[\\n\\\\omega^2 = m^2 (A^2 - 4) \\\\geq 0.\\n\\\\]', '\\\\begin{equation*}\\nm = \\\\int_{i=1}^{9-p} n^{i} \\\\left[ \\\\sum_{j=1}^{9-p} \\\\left( \\\\partial_{j} h_{ij} - \\\\partial_{i} h_{jj} \\\\right) - \\\\sum_{a=1}^{p} \\\\partial_{i} h_{aa} \\\\right] r^{8-p} d \\\\Omega,\\n\\\\end{equation*}', 'The LaTeX representation for the image you provided is:\\n\\n```latex\\n\\\\begin{equation}\\ng_{\\\\mu\\\\nu} = (e_{\\\\mu}^a)^* \\\\eta_{\\\\mu\\\\nu} * e^b\\n\\\\end{equation}\\n```', '```latex\\nk \\\\left( 2 \\\\pi \\\\right) ^ {8} \\\\int _ { 0 } ^ { 2 0 } r _ { i n i } ^ { 4 } \\\\left( R _ { c } \\\\right) ^ { 6 } .\\n```', '\\\\begin{equation*}\\n\\\\Gamma _ { 1 } ^ { 2 } : R = S ( T ) ^ { 2 } , F = L ^ { - 1 } \\\\, ,\\n\\\\end{equation*}', '```latex\\nM^2_H = H + Ic_1.79_78 + Ic_2.76_77 + I_v \\\\gamma_7.\\n```', '\\\\begin{equation*}\\nD_{2}(G,N) = \\\\delta(g_{0} + g_{1} + g_{2} - \\\\rho)(G(N - 3, 3)).\\n\\\\end{equation*}', '\\\\[\\n\\\\Theta _ { \\\\alpha } (1) = X _ { \\\\alpha \\\\beta \\\\phi } ^ { i j } \\\\Theta _ { \\\\beta } ^ { j }\\n\\\\]', '\\\\begin{equation*}\\n\\\\frac{\\\\partial x}{2m} + \\\\frac{(\\\\partial x)_S^2}{2m} - \\\\frac{h^2}{2m} \\\\frac{\\\\partial x^2}{\\\\sqrt{\\\\rho}} = -V,\\n\\\\end{equation*}', '\\\\begin{equation}\\n[A_{\\\\mu \\\\nu}, [A_{\\\\mu \\\\nu}, A_{\\\\nu}]] = -\\\\bar{\\\\psi} \\\\Gamma \\\\mu \\\\nu \\\\psi.\\n\\\\end{equation}', '\\\\[\\n\\\\hat{F}(\\\\beta) = \\\\frac{1}{2} f(0|\\\\beta) = \\\\frac{1}{\\\\mu} \\\\int_{0}^{\\\\infty} \\\\hat{\\\\Phi}(\\\\omega) d\\\\omega \\\\ln{(1 - e^{-\\\\beta \\\\omega})},\\n\\\\]', '\\\\begin{equation*}\\n(\\\\Delta - Q)_n = \\\\sqrt{1 + \\\\frac{4\\\\pi g_4 N^2}{Q^2}}.\\n\\\\end{equation*}', '\\\\begin{equation*}\\n\\\\mathcal{A} = 128 \\\\pi^4 (1 + H(r - x)),\\n\\\\end{equation*}', '\\\\begin{equation*}\\n\\\\Omega _ { \\\\sigma } ^ { I } = \\\\frac { 1 } { 2 } < d l _ { \\\\sigma } \\\\, , \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\, \\\\,', '\\\\[\\n\\\\pi_a = (\\\\partial_0 + i A_0) z_a^*, \\\\pi_b = (\\\\partial^2 - i A^2) z_b\\n\\\\]', '\\\\begin{align*}\\n\\\\Delta H &= \\\\frac{N^3 4^2 2^{-1/4}}{5 \\\\sqrt{3} \\\\pi^3} \\\\frac{g_a (M_s L)^{1/2}}{\\\\beta^{1/2} (2 \\\\pi R_M)^{9/2}} = 1.91 \\\\times 10^{-5},\\n\\\\end{align*}', '\\\\[\\n\\\\Delta \\\\phi + \\\\frac{\\\\alpha^2}{2} \\\\left( \\\\frac{\\\\partial \\\\lambda}{\\\\alpha} \\\\right)^2 - \\\\frac{\\\\alpha h^2}{2} \\\\left( \\\\frac{\\\\partial \\\\alpha}{\\\\alpha} \\\\right)^2 = 0.\\n\\\\]', '\\\\begin{equation*}\\nS = \\\\int d^{4}x \\\\sqrt{-g} \\\\left[ \\\\tilde{R} - \\\\frac{1}{2} \\\\left( \\\\tilde{\\\\nabla} \\\\Phi \\\\right)^{2} - \\\\frac{1}{2} \\\\left( \\\\tilde{\\\\nabla} y \\\\right)^{2} - \\\\Lambda_{ME} e^{2 \\\\Phi} \\\\cosh \\\\left( \\\\sqrt{3} y \\\\right) \\\\right],\\n\\\\end{equation*}', '\\\\begin{equation*}\\n|\\\\Psi_B \\\\rangle = N \\\\exp \\\\left( -\\\\frac{1}{2} \\\\sum_{m,n=1}^{\\\\infty} a_m^+ V_{mn}^B a_n^+ \\\\right) |\\\\Omega \\\\rangle\\n\\\\end{equation*}', '\\\\[\\n\\\\langle \\\\Omega | \\\\phi^{(h)}(x) \\\\phi^{(h)}(w) | \\\\Omega \\\\rangle = \\\\sum_{n=0}^{\\\\infty} w^n x^{-2h-n} \\\\phi^{(h,0,h)}_{-h-n} \\\\phi^{(h,h-n)}_{h-n}\\n\\\\]', '\\\\[\\n\\\\omega v(f(u)) = u \\\\pi (u),\\n\\\\]', '\\\\begin{equation*}\\nx_1 x_2 x_3 x_4 = x_1 x_3 + x_2 x_4.\\n\\\\end{equation*}', '```latex\\nQ^{ab} = (P^{-1})_{c}^{e} \\\\{ \\\\chi^{a}, \\\\chi^{d} \\\\} (P^{-1})_{b}^{d}\\n```', '\\\\[\\nI_n^{\\\\mu\\\\nu} = \\\\frac{1}{2} \\\\sum_k \\\\alpha_k^\\\\mu \\\\alpha_{n-k}^\\\\nu \\\\eta_{\\\\mu\\\\nu}\\n\\\\]', '```latex\\n\\\\documentclass{article}\\n\\\\usepackage{amsmath}\\n\\\\begin{document}\\n\\\\[ \\\\mathcal{M} = \\\\int d\\\\phi d\\\\psi \\\\Psi^* \\\\]\\n\\\\end{document}\\n```', 'The LaTeX representation for the image you provided is:\\n\\n```latex\\nF = (l_1 - l_1^2) K l_2 - (l_2 - l_2^2) K l_1\\n```']\n"
     ]
    }
   ],
   "source": [
    "model.disable_adapters() # Disable adapters\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128, use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "model.enable_adapters() # Enable adapters back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the baseline quantized model does not perform well in predicting the correct latex equation.\n",
    "\n",
    "Now let's set the LoRA adapter from epoch 3 as the active adapter and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['( 5 . 1 7 ) = - \\\\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\\\left\\\\{ \\\\frac { 1 } { 4 } \\\\frac { 1 } { \\\\lambda } z ^ { 2 } \\\\lambda ^ { 2 } + \\\\frac { 1 } { 2 } \\\\ln ( z ) \\\\right\\\\}']\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle ( 5 . 1 7 ) = - \\frac { ( - i ) ^ { m + n - 1 } } { ( m + n - 1 ) ! } z ^ { m + n - 1 } \\left\\{ \\frac { 1 } { 4 } \\frac { 1 } { \\lambda } z ^ { 2 } \\lambda ^ { 2 } + \\frac { 1 } { 2 } \\ln ( z ) \\right\\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.set_adapter(\"epoch_3\")  # Set the adapter to the last epoch\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128, use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "display(Math(output_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model performs well on the test sample !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Let's evaluate the model on test dataset to see how well our model performs on the test set. We will evaluate our model using the following Metric:\n",
    "1. [Bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
    "2. [Levenshtein edit distance](https://medium.com/@ethannam/understanding-the-levenshtein-distance-equation-for-beginners-c4285a5604f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary whitespace from LaTeX code.\n",
    "import re\n",
    "def post_process(s: str):\n",
    "    text_reg = r'(\\\\(operatorname|mathrm|text|mathbf)\\s?\\*? {.*?})'\n",
    "    letter = '[a-zA-Z]'\n",
    "    noletter = '[\\W_^\\d]'\n",
    "    names = [x[0].replace(' ', '') for x in re.findall(text_reg, s)]\n",
    "    s = re.sub(text_reg, lambda match: str(names.pop(0)), s)\n",
    "    news = s\n",
    "    while True:\n",
    "        s = news\n",
    "        news = re.sub(r'(?!\\\\ )(%s)\\s+?(%s)' % (noletter, noletter), r'\\1\\2', s)\n",
    "        news = re.sub(r'(?!\\\\ )(%s)\\s+?(%s)' % (noletter, letter), r'\\1\\2', news)\n",
    "        news = re.sub(r'(%s)\\s+?(%s)' % (letter, noletter), r'\\1\\2', news)\n",
    "        if news == s:\n",
    "            break\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate Baseline model, disable adapter first (Uncomment the line below)\n",
    "# model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [04:50<00:00, 12.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.8194071094627821\n",
      "Average Normalized Edit Distance: 0.09973906130739238\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# Initialize evaluation tools\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "norm_edit_dists = []\n",
    "\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "batch_size = 64\n",
    "n_samples = len(converted_test_dataset)\n",
    "\n",
    "for start_idx in tqdm(range(0, n_samples, batch_size), desc=\"Evaluating\"):\n",
    "    batch = converted_test_dataset[start_idx : start_idx + batch_size]\n",
    "\n",
    "    batch_texts = []\n",
    "    batch_images = []\n",
    "    batch_ground_truths = []\n",
    "\n",
    "    for sample in batch:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": instruction}\n",
    "            ]}\n",
    "        ]\n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        batch_texts.append(input_text)\n",
    "\n",
    "        image_inputs, _ = process_vision_info(sample)\n",
    "        batch_images.append(image_inputs)\n",
    "\n",
    "        gt = sample[1]['content'][0]['text']\n",
    "        batch_ground_truths.append(gt)\n",
    "\n",
    "    # Process batch\n",
    "    inputs = processor(\n",
    "        text=batch_texts,\n",
    "        images=batch_images,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    batch_outputs = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    for pred, true in zip(batch_outputs, batch_ground_truths):\n",
    "        all_predictions.append(pred)\n",
    "        all_references.append([true])  # wrap for BLEU\n",
    "\n",
    "        norm_dist = Levenshtein.distance(post_process(pred), post_process(true)) / max(len(pred), len(true))\n",
    "        norm_edit_dists.append(norm_dist)\n",
    "    \n",
    "# Final evaluation\n",
    "bleu_result = bleu_metric.compute(predictions=all_predictions, references=all_references)\n",
    "average_edit_distance = sum(norm_edit_dists) / len(norm_edit_dists)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"BLEU Score:\", bleu_result['bleu'])\n",
    "print(\"Average Normalized Edit Distance:\", average_edit_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73.9M/73.9M [00:01<00:00, 47.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_nf4_epoch3/commit/dc9169094ab0106d63fe7ae244b9070e0488b4fe', commit_message='Upload Qwen2VLForConditionalGeneration', commit_description='', oid='dc9169094ab0106d63fe7ae244b9070e0488b4fe', pr_url=None, repo_url=RepoUrl('https://huggingface.co/arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_nf4_epoch3', endpoint='https://huggingface.co', repo_type='model', repo_id='arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_nf4_epoch3'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"arunmadhusudh/qwen2_VL_2B_LatexOCR_qlora_nf4_epoch3\", token = \"your_hf_token_here\") # Online saving\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
